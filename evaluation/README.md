# Evaluation of Pipeline

Here we have several scripts to evaluate the pipeline on a test dataset.

Follow the following steps for this.

## Create true annotations

Create a file true annotations. For this just run the script `evaluation_file_gen.py`. For the argument `--path` specify the path to the test_images folder. Make sure the images are not directly in this folder but in the subdirectory `test_images/images`.

```shell
python evaluation/evaluation_file_gen.py --path path/to/test_images
```

This will create a skeleton json file where for each image you manually fill out the reading, the unit and the range of the gauge. The range is there to compute the relative error. For this specify the absolute length of the scale. For example if the scale goes from -2 to 5, the range would be 7.

## Run the test images through the pipelines to get results

For this use the command to execute the pipeline:

```shell
python pipeline.py --detection_model path/to/detection_model --segmentation_model /path/to/segmentation_model --key_point_model path/to/key_point_model --base_path path/to/results --debug --eval --input path/to/test_image_folder/images
```

This will create a run folder into the results folder specified. In the run folder you get one folder per image.

## Run evaluation script

Once we have both the true annotations and the results from the pipeline we can compare them with the evaluation script:

```shell
python evaluation/evaluation.py --run_path path/to/run_path --true_readings_path path/to/true_readings_file
```

The `run_path` here is the folder generated by the `pipeline.py` script.

Running this script does two things. For one it summarizes the individual results of the images in the run folder into a single file it saves into the run folder.
Secondly it generates a file called `evaluation.json` in the run folder which summarizes the results.

## Enhance test set

For our experiments we enhanced the set of test images by rotatating them randomly. We do this with the notebook `rotate_test_imgs.ipynb`.

## Full evaluation

First label your data set with `label-studio` <https://labelstud.io/>. For this you have to create 3 different projects, one for key points, one for bounding boxes and one for segmentation. For segmentation choose labels with polygons. The name of the labels is important and can be found in the reference examples provided.

When exporting these with label-studio you get 3 json files, one for each project. Provide these when running the full_evaluation script.

To run the full_evaluation script execute:

```shell
python evaluation/full_evaluation.py --bbox_true_path path/to/bbox.json --keypoint_true_path path/to/keypoint.json --segmentation_true_path path/to/seg.json ----run_path path/to/run_path
```
